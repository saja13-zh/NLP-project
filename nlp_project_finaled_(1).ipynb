{
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saja13-zh/NLP-project/blob/main/nlp_project_finaled_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **load the dataset and preview it**"
      ],
      "metadata": {
        "id": "X-8-CbwvID28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqPVTxjRKhcT",
        "outputId": "7f776419-ed86-4b42-eb38-c42d1265826d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"hard\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:18:59.755996Z",
          "iopub.execute_input": "2024-04-23T13:18:59.756256Z",
          "iopub.status.idle": "2024-04-23T13:19:06.446907Z",
          "shell.execute_reply.started": "2024-04-23T13:18:59.756233Z",
          "shell.execute_reply": "2024-04-23T13:19:06.445928Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OSGLM9BID3C",
        "outputId": "5b5e3080-9066-4d0e-b63c-d6bf75079efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'].data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:19:06.448666Z",
          "iopub.execute_input": "2024-04-23T13:19:06.449105Z",
          "iopub.status.idle": "2024-04-23T13:19:06.457082Z",
          "shell.execute_reply.started": "2024-04-23T13:19:06.449078Z",
          "shell.execute_reply": "2024-04-23T13:19:06.456386Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOwa3s7ZID3G",
        "outputId": "5837461b-d4fb-45d2-a1af-10bceb6024a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MemoryMappedTable\n",
              "text: string\n",
              "label: int64\n",
              "----\n",
              "text: [[\"“ممتاز”. النظافة والطاقم متعاون. \",\"استثنائي. سهولة إنهاء المعاملة في الاستقبال. لاشيئ\",\"استثنائي. انصح بأختيار الاسويت و بالاخص غرفه رقم 801. نوعية الارضيه\",\"“استغرب تقييم الفندق كخمس نجوم”. لا شي. يستحق 2 نجمه \",\"جيد. المكان جميل وهاديء. كل شي جيد ونظيف بس كان حوض السباحه لايعمل في هذي الفتره حسب كلامهم يقولوا فيه صيانه والله اعلم\",...,\"“غير جيد وغالي”. . الغرف ضيقة والمصاعد قديمه واثاث المنتجع قديم\",\"“كنت أتمنى الأفضل”. لاشيء. عدم المام موظفي الاستقبال بالمعلومات الكافية ولغتهم العربية ركيكة\",\"“للراحة والاسترخاء والاستمتاع.. سراي عجمان هو المفتاح...”. شكرًا على لطافتكم... بالفعل استمتعت بكل دقيقة .. بالتأكيد لن تكون الزيارة الاولى ... وسوف اعتمدها كمكان  لراحتي ووقت للاسترخاء ... شكرًا جدا على التعامل الراقي من قبل جميع الموظفين... وشكراً على الخدمات المنفردة عن غيرها...لي عودة فالقريب العاجل... تحياتي... \",\"“كانت اجازه جدا جميله”. ماشالله كل شي فيه حلو وخصوصي انو عائلي ممتاز. \",\"استثنائي. كل شي ممتاز من خدمة ونظافة وطعام. \"],[\"“انصح به”. اعجبني الهدوء والطاقم ممتاز والي اكثر من هذا اللايف جارد الي على المسبح. \",\"“تقييمي للفندق : موقع رائع خدمات رائعة لكن السعر عالي جدا”. اعجبني توافر الخدمات وموقع الفندق بالقرب من الشاطيء .. غلاء سعر الغرفة  لليلة الواحدة ...وغلاء المأكولات في مطاعم الفندق ..\",\"“ممتاز”. كل شي. لا يوجد\",\"“ممتاز جدا”. نظيف .. وراقي جدا ... \",\"استثنائي. فندق نظيف وممتاز وحسن استفبال وخذمه ممتازه. كل شي ممتاز\",...,\"“جيدة لولا ازعاج صالة الافراح بالدور التاسع”. الموقع قريب من البلد والاسواق. وجود صالة زواج بالدور التاسع وبالتالي تتسبب في ازعاج النائمين وعدم راحتهم\",\"ضعيف. نعم. اعمال صيانه وقت الظهر وكذلك دورات المياة مهترئه جدا جدا\",\"“فندق جميل”. اعجبني النظافه والهدؤ متوفر فيه كل سبل الراحه. لا شي سوا الافطار كان اقل من العادي ارجو الاهتمام اكثر\",\"ضعيف. . الفرش قديم جدا ودورة المياه سيئة جدا\",\"“انصح به للعائلات”. بالنسبه لتعليقات الاخوه ياريت يكون عندكم امانه وذمه قبل التعليق حجزت ولقيت عكس الكلام المكتوب الفندق جميل والغرف رائعه والاثاث جديد والمبنى جميل عجبني المسبج والجلسات والملعب يصلح للعائلات. الخدمات : السوبرماركت والصراف بعيد\"],...,[\"“تقييم فندق”. اللوبي فقط. النظافة متدنية الأثاث قذر جداً جداً جداً قديم لم أستطيع الجلوس عليه بدون حاجز من شدة القذارة غطاء الأسرة لم يتم تغييرها طول مدة أقامتنا أدوات المطبخ لاتفي بالاحتياجات لا يوجد  توستر لاشي في هذا الفندق يشجع للعودة مره اخرى سئ في كل شي\",\"جيد. قريب من محطة القطار. الهدؤ سعرة مناسب. بعيد عن وسط المدينةالمطاعم خارج الفندق قليلة البوفية بالفندق سعره غالي المصاعد تتأخر لا يتم توفير سرير اضافي راقي\",\"مخيب للأمل. طريقة النصب. الواي فاي والسعر مبالغ فيه\",\"جيد جداً. المرونة. الإضاءة منخفضة\",\"“فندق ممتاز نوعا ما”. موقع الفندق ممتاز. \",...,\"استثنائي. . صغر حجم الغرف\",\"ممتاز. . لم يتم توفير سرير لطفل رضيع حيث تم الطلب مسبقا\",\"ممتاز. قرب المكان من الحرم ونظافة الفندق وحسن الاستقبال. لا يوجد\",\"“الروح الجميلة بالمؤظف احمد السالمي الذي يجتهد لراحة النزلاء..له تحياتي”. . عدم التجاوب مع طلب النزيل بشكل سريع!! فتتصل فيتاخر المأمور من الاجابه عليك..\",\"مخيب للأمل. . الخدمة سيئة جدا جدا جدا من مطبخ الأوتيل ومن خدمات الغرف طلبنا تمر وقهوة جانا تمر خربانتفاح خربان\"],[\"“دون المستوى”. الموقع. بطئ في الخدمات\",\"ممتاز. القرب والنظافة. السعر مبالغ فيه حجزت جناح ب ١٢٠٠\",\"ضعيف جداً. ولا شي. النظافه والاستقبال و البوفيه نصيحه لأحد يفكر يقيم في هذا الفندق\",\"“خدمة ونظافة مميزة”. النظافة. الافطار\",\"“الخدمه”. . لم تعجبني خدمه الغرف وﻻ يوجد هناك ضيافه كالفنادق.. وعند الاتصال بالخدمات الخدمه ﻻتصل اليك الا بعد يومين .. مثل الماء والكوفي والشاي وغيرها من الخدمات المفروض توافرها في الغرفه فقد اثار ذلك غضبي واستيائي وافراد اسرتي\",...,\"“فند”. لا شئ عجبني. طقم العمل سيئ جدالا يوجد بالفندق اي خدمات الحمامات و الغرفة غير نظيفة بالمرة و رائحة كريهة و يوجد حشرات بالحمام\",\"“سيئ”. قربه من المسجد النبوي الشريف. استخدام موظف الإستقبال للمواقف المخصصة للعملاء وحجز سياراتهم بسيارته دون مراعاة لأي طارئ أو مسؤولية\",\"“اسوأ إقامة في الرحلة !”. القرب من الحرم. كل شيء ازعاج عدم تجاوب الإستقبال الأثاث قديم ليست نظيفة زحام شديد جدا على المصاعد!باب الغرفة يفتح بصعوبة تأخر في اجراءات الدخول غرفة كانت غير نظيفة والجيران ازعاجهم شديدطلبت تغييرها غيروها بغرفة بينها وبين الغرفة المجاورة باب فيه فتحة لايمكن الإطمئنان بسببها ( يمكن من خلالها رؤية من يمر إلى دورة المياة )باختصار ندمت أني سكنت فيها !ولن أكررها بإذن الله\",\"“دون المستوى”. قربه من الحرم. كل شيء\",\"مخيب للأمل. . سوء التنسيق من حيث معرفة الغرف بها زوار ام فارغة..حتى ان ادارة الفندق كل ساعتين يرسلوا احد العمال ليتاكد من خلو الغرفة من عدمه\"]]\n",
              "label: [[1,4,4,0,3,...,1,0,4,4,4],[4,3,4,4,4,...,1,1,3,1,4],...,[1,3,1,3,3,...,4,4,4,3,1],[1,4,0,4,1,...,0,1,1,1,1]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**split the dataset into text and labels**"
      ],
      "metadata": {
        "id": "fbsHeGpnID3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = dataset['train'].data['text']\n",
        "labels = dataset['train'].data['label']\n",
        "\n",
        "for i in range(min(5, len(text_data))):\n",
        "    print(f\"Text data {i+1}: {text_data[i]}\")\n",
        "    print(f\"Labels {i+1}: {labels[i]}\")\n",
        "    print(\"----------------------\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:19:06.458132Z",
          "iopub.execute_input": "2024-04-23T13:19:06.458485Z",
          "iopub.status.idle": "2024-04-23T13:19:06.496957Z",
          "shell.execute_reply.started": "2024-04-23T13:19:06.458453Z",
          "shell.execute_reply": "2024-04-23T13:19:06.496059Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgdtHVBSID3H",
        "outputId": "b25f95dd-74ae-47fe-83d0-e68dfa455fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text data 1: “ممتاز”. النظافة والطاقم متعاون. \n",
            "Labels 1: 1\n",
            "----------------------\n",
            "Text data 2: استثنائي. سهولة إنهاء المعاملة في الاستقبال. لاشيئ\n",
            "Labels 2: 4\n",
            "----------------------\n",
            "Text data 3: استثنائي. انصح بأختيار الاسويت و بالاخص غرفه رقم 801. نوعية الارضيه\n",
            "Labels 3: 4\n",
            "----------------------\n",
            "Text data 4: “استغرب تقييم الفندق كخمس نجوم”. لا شي. يستحق 2 نجمه \n",
            "Labels 4: 0\n",
            "----------------------\n",
            "Text data 5: جيد. المكان جميل وهاديء. كل شي جيد ونظيف بس كان حوض السباحه لايعمل في هذي الفتره حسب كلامهم يقولوا فيه صيانه والله اعلم\n",
            "Labels 5: 3\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**data cleaning use tnkeeh**"
      ],
      "metadata": {
        "id": "LseDih2xID3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tnkeeh"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:19:06.499572Z",
          "iopub.execute_input": "2024-04-23T13:19:06.500086Z",
          "iopub.status.idle": "2024-04-23T13:19:19.884767Z",
          "shell.execute_reply.started": "2024-04-23T13:19:06.500060Z",
          "shell.execute_reply": "2024-04-23T13:19:19.883586Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au6LXUJaID3I",
        "outputId": "5f7a18da-0ab8-4e43-8108-e22ea76e3eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tnkeeh in /usr/local/lib/python3.10/dist-packages (0.0.9)\n",
            "Requirement already satisfied: farasapy in /usr/local/lib/python3.10/dist-packages (from tnkeeh) (0.0.14)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from tnkeeh) (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->tnkeeh) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tnkeeh) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tnkeeh) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tnkeeh) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tnkeeh) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tnkeeh) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->tnkeeh) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets->tnkeeh) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->tnkeeh) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->tnkeeh) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->tnkeeh) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->tnkeeh) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->tnkeeh) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->tnkeeh) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->tnkeeh) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->tnkeeh) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tnkeeh as tn\n",
        "\n",
        "cleaner = tn.Tnkeeh(remove_diacritics = True,remove_special_chars=True,remove_english=True,\n",
        "                 normalize=True,remove_tatweel=True,remove_repeated_chars=True,remove_html_elements=True,remove_links=True,remove_twitter_meta=True,remove_long_words=True   )\n",
        "\n",
        "cleaned_dataset = cleaner.clean_hf_dataset(dataset, 'text')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:19:19.886325Z",
          "iopub.execute_input": "2024-04-23T13:19:19.886752Z",
          "iopub.status.idle": "2024-04-23T13:19:56.549866Z",
          "shell.execute_reply.started": "2024-04-23T13:19:19.886712Z",
          "shell.execute_reply": "2024-04-23T13:19:56.548950Z"
        },
        "trusted": true,
        "id": "4FzOvrvaID3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(min(5, len(cleaned_dataset['train']['text']))):\n",
        "    print(f\"cleaned dataset {i+1}: {cleaned_dataset['train']['text'][i]}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:19:56.551152Z",
          "iopub.execute_input": "2024-04-23T13:19:56.551458Z",
          "iopub.status.idle": "2024-04-23T13:19:58.074683Z",
          "shell.execute_reply.started": "2024-04-23T13:19:56.551410Z",
          "shell.execute_reply": "2024-04-23T13:19:58.073785Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8wXdCx3ID3J",
        "outputId": "2ce3168c-7fe0-4238-e807-e6832d79097f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleaned dataset 1:  ممتاز النظافة والطاقم متعاون \n",
            "cleaned dataset 2: استثنائي سهولة إنهاء المعاملة في الاستقبال لاشيئ\n",
            "cleaned dataset 3: استثنائي انصح بأختيار الاسويت و بالاخص غرفه رقم 801 نوعية الارضيه\n",
            "cleaned dataset 4:  استغرب تقييم الفندق كخمس نجوم لا شي يستحق 2 نجمه \n",
            "cleaned dataset 5: جيد المكان جميل وهاديء كل شي جيد ونظيف بس كان حوض السباحه لايعمل في هذي الفتره حسب كلامهم يقولوا فيه صيانه والله اعلم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace all forms of letter alef-with-hamza (أ، إ، آ) with the normalized form (ا).\n",
        "and (ة) with (ه)"
      ],
      "metadata": {
        "id": "R_a0450-ID3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeArabic(text):\n",
        "    if isinstance(text, str):\n",
        "        normalized_text = text.replace(\"إ\", \"ا\").replace(\"أ\", \"ا\").replace(\"آ\", \"ا\")\n",
        "        normalized_text = normalized_text.replace(\"ة\", \"ه\").replace(\"ى\", \"ي\")\n",
        "        normalized_text = normalized_text.replace(\"ؤ\", \"ء\")\n",
        "        return normalized_text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "\n",
        "normalized_texts = [normalizeArabic(str(text)) for text in cleaned_dataset['train']['text']]\n",
        "\n",
        "for i in range(min(5, len(normalized_texts))):\n",
        "    print(f\"normalized dataset {i+1}: {normalized_texts[i]}\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:19:58.076282Z",
          "iopub.execute_input": "2024-04-23T13:19:58.076591Z",
          "iopub.status.idle": "2024-04-23T13:19:58.510148Z",
          "shell.execute_reply.started": "2024-04-23T13:19:58.076565Z",
          "shell.execute_reply": "2024-04-23T13:19:58.509227Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18pNWXdZID3L",
        "outputId": "51ab6810-6042-4975-ab69-87d9216eee78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalized dataset 1:  ممتاز النظافه والطاقم متعاون \n",
            "normalized dataset 2: استثنائي سهوله انهاء المعامله في الاستقبال لاشيئ\n",
            "normalized dataset 3: استثنائي انصح باختيار الاسويت و بالاخص غرفه رقم 801 نوعيه الارضيه\n",
            "normalized dataset 4:  استغرب تقييم الفندق كخمس نجوم لا شي يستحق 2 نجمه \n",
            "normalized dataset 5: جيد المكان جميل وهاديء كل شي جيد ونظيف بس كان حوض السباحه لايعمل في هذي الفتره حسب كلامهم يقولوا فيه صيانه والله اعلم\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK's stopwords for Arabic\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the stopwords for Arabic\n",
        "stopwords_arabic = set(stopwords.words('arabic'))\n",
        "#remove what needed from stop words\n",
        "want_word={'لا','نعم','ليس','إلا','كلا','ولا'}\n",
        "stopwords=[word for word in stopwords_arabic if word not in want_word]\n",
        "print(stopwords)\n",
        "print(len(stopwords))\n",
        "print(len(stopwords_arabic))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:19:58.511682Z",
          "iopub.execute_input": "2024-04-23T13:19:58.511933Z",
          "iopub.status.idle": "2024-04-23T13:19:59.529675Z",
          "shell.execute_reply.started": "2024-04-23T13:19:58.511911Z",
          "shell.execute_reply": "2024-04-23T13:19:59.528755Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZJmk4wnID3M",
        "outputId": "0207ef34-6f3b-4f9d-ea0e-6b08cbfa251e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['يناير', 'لكيلا', 'طفق', 'ثمانمئة', 'خمسمئة', 'الآن', 'عوض', 'كلما', 'اللائي', 'ذو', 'ف', 'فلس', 'غالبا', 'تارة', 'ذهب', 'تلكما', 'حيَّ', 'بَلْهَ', 'أنبأ', 'لات', 'كلّما', 'كأي', 'لولا', 'أيا', 'كأن', 'حسب', 'هاتان', 'بكما', 'أحد', 'شمال', 'مثل', 'ثمانين', 'لبيك', 'ساء', 'هذه', 'هن', 'ثاني', 'ذيت', 'هنا', 'ريال', 'غين', 'تفعلون', 'حتى', 'إياه', 'إزاء', 'وجد', 'خامس', 'عشرة', 'حبذا', 'جويلية', 'تسع', 'كاف', 'علق', 'هَاتِه', 'بئس', 'لسنا', 'هلم', 'كلتا', 'لهم', 'به', 'حقا', 'ة', 'أخذ', 'أيضا', 'هنالك', 'بك', 'خمسين', 'جلل', 'ارتدّ', 'بهما', 'ع', 'دونك', 'هَذَيْنِ', 'أقل', 'عَدَسْ', 'كى', 'ابتدأ', 'يورو', 'ألفى', 'أخٌ', 'اللواتي', 'ئ', 'بضع', 'شباط', 'فيه', 'خاصة', 'أنتما', 'ين', 'باء', 'الذي', 'يفعلان', 'أجمع', 'خمس', 'ض', 'مائة', 'إذما', 'تلك', 'خميس', 'لك', 'ومن', 'طَق', 'سبعة', 'خمسة', 'حادي', 'غدا', 'كسا', 'إي', 'سادس', 'صبر', 'أينما', 'ذي', 'بعد', 'سرعان', 'تحت', 'تين', 'خلف', 'كلَّا', 'نون', 'خمسمائة', 'جعل', 'ليست', 'أنتِ', 'عند', 'معاذ', 'مكانكنّ', 'ما أفعله', 'تاسع', 'كما', 'عما', 'خاء', 'فيها', 'صراحة', 'ما', 'ؤ', 'تشرين', 'عاد', 'درى', 'وا', 'هاء', 'ستمئة', 'أمد', 'أوشك', 'لاسيما', 'ش', 'أكثر', 'ي', 'إليكنّ', 'سمعا', 'إلَيْكَ', 'منذ', 'ثلاث', 'مهما', 'لمّا', 'جانفي', 'آب', 'حاء', 'ذلكن', 'لكم', 'حزيران', 'ط', 'تِه', 'أنًّ', 'اثنا', 'قاف', 'مع', 'أغسطس', 'ثامن', 'فيما', 'أنّى', 'سبتمبر', 'أفريل', 'هناك', 'انبرى', 'ثمّ', 'بات', 'إياهم', 'ألف', 'مئة', 'ث', 'قرش', 'مكانكما', 'مذ', 'فبراير', 'زعم', 'ما انفك', 'أهلا', 'وإذا', 'تفعلين', 'قام', 'إنما', 'ياء', 'إياهن', 'سبع', 'ر', 'كأيّ', 'بَسْ', 'لي', 'عامة', 'واحد', 'هم', 'لستن', 'بعض', 'لهما', 'شرع', 'قاطبة', 'عشرين', 'ثم', 'إحدى', 'مازال', 'سبت', 'سبعمائة', 'مافتئ', 'م', 'إن', 'صار', 'أولئك', 'غير', 'أقبل', 'لدن', 'تلكم', 'مكانكم', 'شتان', 'آمينَ', 'مليم', 'إذن', 'فلا', 'بطآن', 'لعمر', 'أربعاء', 'أيّ', 'مه', 'آها', 'ذواتي', 'ثلاثة', 'وإن', 'رويدك', 'ليسوا', 'عدَّ', 'حيثما', 'أيها', 'بكم', 'آذار', 'من', 'أضحى', 'حرى', 'ذِه', 'راء', 'هذين', 'طاق', 'كيف', 'لوما', 'لدى', 'لها', 'آ', 'أو', 'ممن', 'هكذا', 'أل', 'هيت', 'أوّهْ', 'آهاً', 'كاد', 'انقلب', 'الألى', 'ثلاثون', 'ن', 'فإذا', 'آهٍ', 'حَذارِ', 'أمسى', 'ستون', 'أبريل', 'هذان', 'نَخْ', 'إليك', 'ذينك', 'أما', 'أنت', 'واو', 'هو', 'ذان', 'خمسون', 'أعطى', 'بهم', 'فو', 'أربعة', 'تحوّل', 'هذا', 'بنا', 'تاء', 'هيّا', 'كان', 'تخذ', 'خال', 'بل', 'إليكن', 'هاهنا', 'هَؤلاء', 'بعدا', 'ثمة', 'صدقا', 'ثلاثمئة', 'هَذِي', 'ا', 'ولكن', 'ماي', 'هلّا', 'تسعمئة', 'مئتان', 'هاته', 'إما', 'أرى', 'سين', 'ظ', 'واهاً', 'كثيرا', 'أربع', 'إياك', 'عسى', 'رزق', 'أفعل به', 'أول', 'مادام', 'فرادى', 'ثلاثمائة', 'تسعين', 'سنتيم', 'اربعون', 'ص', 'خلافا', 'درهم', 'دينار', 'ذين', 'طرا', 'اخلولق', 'ت', 'غ', 'لسن', 'كن', 'ثمانون', 'مساء', 'ثان', 'لا سيما', 'ح', 'إنه', 'بي', 'إليكم', 'تعسا', 'كأنّ', 'كرب', 'ء', 'إنَّ', 'ستة', 'أبو', 'جنيه', 'أربعمائة', 'بمن', 'بها', 'لهن', 'أنى', 'آي', 'راح', 'صاد', 'أولالك', 'بيد', 'حجا', 'بسّ', 'جيم', 'تِي', 'كذلك', 'هاتين', 'عل', 'وهو', 'صباح', 'هاتي', 'يوليو', 'مرّة', 'اثنان', 'أين', 'بهن', 'الألاء', 'تانِك', 'تفعلان', 'ظنَّ', 'أمس', 'وهب', 'اللذان', 'تينك', 'وما', 'ه', 'أعلم', 'عجبا', 'ماذا', 'هَذانِ', 'جوان', 'ذانِ', 'ذا', 'خ', 'لن', 'إيانا', 'آناء', 'اللتان', 'لنا', 'هما', 'هبّ', 'مايو', 'ثمان', 'شتانَ', 'ثمنمئة', 'ذواتا', 'أمام', 'لكنما', 'ليسا', 'تسعة', 'ترك', 'هي', 'قبل', 'حيث', 'ثماني', 'ضاد', 'اثني', 'نفس', 'ثمانية', 'تانِ', 'حدَث', 'رأى', 'ألا', 'ذلكما', 'ذه', 'ذوا', 'يفعلون', 'سبعون', 'ستين', 'أصبح', 'لكنَّ', 'رجع', 'فمن', 'إى', 'تلقاء', 'ليت', 'بلى', 'هَاتانِ', 'ريث', 'هَجْ', 'هاك', 'أمامك', 'كذا', 'أخبر', 'خبَّر', 'ثلاثين', 'أبٌ', 'ليستا', 'جمعة', 'ذاك', 'أوت', 'ذِي', 'ست', 'نَّ', 'سبعمئة', 'أبدا', 'قطّ', 'صبرا', 'ذلك', 'بخ', 'همزة', 'نوفمبر', 'إلّا', 'حمو', 'الذين', 'صهْ', 'أيّان', 'ولو', 'رابع', 'بما', 'فلان', 'فيفري', 'وُشْكَانَ', 'لو', 'يونيو', 'أمامكَ', 'إياكم', 'لست', 'شبه', 'إذا', 'تبدّل', 'نيف', 'سحقا', 'ثمّة', 'أيلول', 'ثمَّ', 'تعلَّم', 'حار', 'سقى', 'قلما', 'ته', 'عشرون', 'فوق', 'ذات', 'ل', 'آنفا', 'دال', 'ظلّ', 'سوى', 'أُفٍّ', 'لكي', 'حاي', 'إذ', 'فاء', 'هللة', 'عين', 'إليكَ', 'اربعين', 'اتخذ', 'والذين', 'هَذا', 'عدا', 'إلى', 'زود', 'حمدا', 'أمّا', 'ز', 'ظاء', 'ى', 'ذلكم', 'إياكن', 'جميع', 'أيار', 'عاشر', 'ذَيْنِ', 'وراءَك', 'د', 'ستمائة', 'قد', 'دولار', 'تجاه', 'لكما', 'عليه', 'سرا', 'لعل', 'علم', 'يوان', 'سبحان', 'ورد', 'أنتم', 'بكن', 'اللتين', 'و', 'ها', 'ما برح', 'فإن', 'كيفما', 'لعلَّ', 'شين', 'إذاً', 'أكتوبر', 'حاشا', 'جير', 'أفٍّ', 'غادر', 'هاكَ', 'هيهات', 'والذي', 'حين', 'إياهما', 'وَيْ', 'إيهٍ', 'هَاتَيْنِ', 'ثلاثاء', 'تي', 'كأيّن', 'له', 'شَتَّانَ', 'إياكما', 'إياها', 'أربعمئة', 'لئن', 'أنشأ', 'نحن', 'عليك', 'إيه', 'ذال', 'في', 'عيانا', 'ميم', 'أنا', 'نحو', 'متى', 'أم', 'سابع', 'طاء', 'بين', 'استحال', 'بخٍ', 'أ', 'أف', 'كم', 'أولاء', 'اثنين', 'اللتيا', 'دون', 'غداة', 'لكن', 'عن', 'كي', 'كأين', 'س', 'اللاتي', 'أخو', 'كلاهما', 'خلا', 'بماذا', 'ديسمبر', 'منها', 'بس', 'ب', 'كيت', 'ذ', 'كأنما', 'ّأيّان', 'كانون', 'نا', 'تسعون', 'يمين', 'هيا', 'كِخ', 'أسكن', 'طالما', 'أصلا', 'ضحوة', 'على', 'ثالث', 'مما', 'أنتن', 'إليكما', 'عشر', 'ج', 'آه', 'علًّ', 'إياي', 'كليهما', 'هَذِه', 'أوه', 'نبَّا', 'آض', 'ثاء', 'هَاتِي', 'أي', 'لما', 'أن', 'آهِ', 'يا', 'سبعين', 'كل', 'شيكل', 'ءَ', 'لستم', 'حمٌ', 'أى', 'أجل', 'صهٍ', 'التي', 'لستما', 'دواليك', 'هل', 'وإذ', 'فضلا', 'حبيب', 'ليرة', 'هؤلاء', 'لم', 'ق', 'هلا', 'تموز', 'إنا', 'أطعم', 'هَيْهات', 'بؤسا', 'منه', 'بغتة', 'زاي', 'رُبَّ', 'تسعمائة', 'سوف', 'نيسان', 'مارس', 'اللذين', 'كليكما', 'ك', 'تَيْنِ', 'مكانَك', 'ذانك', 'لام', 'فيم', 'إمّا', 'هذي']\n",
            "695\n",
            "701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **stop word removal**"
      ],
      "metadata": {
        "id": "Rwo3DrEoID3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stopwords]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Apply stopword removal to your normalized texts\n",
        "normalized_texts_without_stopwords = [remove_stopwords(text) for text in normalized_texts]\n",
        "\n",
        "# Print original text and text after stopword removal\n",
        "for i in range(min(5, len(normalized_texts_without_stopwords))):\n",
        "    print(f\"Original Text {i+1}: {normalized_texts[i]}\")\n",
        "    print(f\"Text after Stopword Removal {i+1}: {normalized_texts_without_stopwords[i]}\")\n",
        "    print(\"----------------------\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:19:59.530978Z",
          "iopub.execute_input": "2024-04-23T13:19:59.531326Z",
          "iopub.status.idle": "2024-04-23T13:20:23.442309Z",
          "shell.execute_reply.started": "2024-04-23T13:19:59.531293Z",
          "shell.execute_reply": "2024-04-23T13:20:23.441451Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVp-ClaTID3O",
        "outputId": "1114c0fd-2b1d-4308-9e2f-3af9cb08da34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text 1:  ممتاز النظافه والطاقم متعاون \n",
            "Text after Stopword Removal 1: ممتاز النظافه والطاقم متعاون\n",
            "----------------------\n",
            "Original Text 2: استثنائي سهوله انهاء المعامله في الاستقبال لاشيئ\n",
            "Text after Stopword Removal 2: استثنائي سهوله انهاء المعامله الاستقبال لاشيئ\n",
            "----------------------\n",
            "Original Text 3: استثنائي انصح باختيار الاسويت و بالاخص غرفه رقم 801 نوعيه الارضيه\n",
            "Text after Stopword Removal 3: استثنائي انصح باختيار الاسويت بالاخص غرفه رقم 801 نوعيه الارضيه\n",
            "----------------------\n",
            "Original Text 4:  استغرب تقييم الفندق كخمس نجوم لا شي يستحق 2 نجمه \n",
            "Text after Stopword Removal 4: استغرب تقييم الفندق كخمس نجوم لا شي يستحق 2 نجمه\n",
            "----------------------\n",
            "Original Text 5: جيد المكان جميل وهاديء كل شي جيد ونظيف بس كان حوض السباحه لايعمل في هذي الفتره حسب كلامهم يقولوا فيه صيانه والله اعلم\n",
            "Text after Stopword Removal 5: جيد المكان جميل وهاديء شي جيد ونظيف حوض السباحه لايعمل الفتره كلامهم يقولوا صيانه والله اعلم\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenize text**"
      ],
      "metadata": {
        "id": "FU9HVB7XID3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    tokens =text.split()\n",
        "    return tokens\n",
        "\n",
        "# Tokenize each text after stopword removal\n",
        "tokenized_texts = [tokenize_text(text) for text in normalized_texts_without_stopwords]\n",
        "\n",
        "# Print the tokenized texts\n",
        "for tokens in range(min(5, len(tokenized_texts))):\n",
        "    print(tokenized_texts[tokens])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:23.445271Z",
          "iopub.execute_input": "2024-04-23T13:20:23.445546Z",
          "iopub.status.idle": "2024-04-23T13:20:24.006440Z",
          "shell.execute_reply.started": "2024-04-23T13:20:23.445523Z",
          "shell.execute_reply": "2024-04-23T13:20:24.005500Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DPDYER3ID3P",
        "outputId": "b8866e76-0a9c-4615-f33d-26e2693bde28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ممتاز', 'النظافه', 'والطاقم', 'متعاون']\n",
            "['استثنائي', 'سهوله', 'انهاء', 'المعامله', 'الاستقبال', 'لاشيئ']\n",
            "['استثنائي', 'انصح', 'باختيار', 'الاسويت', 'بالاخص', 'غرفه', 'رقم', '801', 'نوعيه', 'الارضيه']\n",
            "['استغرب', 'تقييم', 'الفندق', 'كخمس', 'نجوم', 'لا', 'شي', 'يستحق', '2', 'نجمه']\n",
            "['جيد', 'المكان', 'جميل', 'وهاديء', 'شي', 'جيد', 'ونظيف', 'حوض', 'السباحه', 'لايعمل', 'الفتره', 'كلامهم', 'يقولوا', 'صيانه', 'والله', 'اعلم']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "clean_texts = [' '.join(text) for text in tokenized_texts]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(clean_texts)\n",
        "# X is a sparse matrix of shape (n_samples, n_features)\n",
        "print(X.shape)\n",
        "print(X[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:24.007728Z",
          "iopub.execute_input": "2024-04-23T13:20:24.008020Z",
          "iopub.status.idle": "2024-04-23T13:20:27.304583Z",
          "shell.execute_reply.started": "2024-04-23T13:20:24.007995Z",
          "shell.execute_reply": "2024-04-23T13:20:27.303656Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJnpBSP1ID3Q",
        "outputId": "e4e383d8-8c4b-4e95-d005-7fc81d3421e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(105698, 118419)\n",
            "  (0, 80972)\t0.5590099036743219\n",
            "  (0, 97045)\t0.6887139971409053\n",
            "  (0, 27718)\t0.33317470269260896\n",
            "  (0, 87022)\t0.319649143940531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(min(5, len(clean_texts))):\n",
        "    print(f\"Clean Text {i+1}: {X[i]}\")\n",
        "    print(\"----------------------\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:27.305889Z",
          "iopub.execute_input": "2024-04-23T13:20:27.306320Z",
          "iopub.status.idle": "2024-04-23T13:20:27.313808Z",
          "shell.execute_reply.started": "2024-04-23T13:20:27.306273Z",
          "shell.execute_reply": "2024-04-23T13:20:27.312855Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iLPqWm7ID3Q",
        "outputId": "e865b9fe-1931-4da7-ea8c-c242b4531a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Text 1:   (0, 80972)\t0.5590099036743219\n",
            "  (0, 97045)\t0.6887139971409053\n",
            "  (0, 27718)\t0.33317470269260896\n",
            "  (0, 87022)\t0.319649143940531\n",
            "----------------------\n",
            "Clean Text 2:   (0, 70196)\t0.49825177596702147\n",
            "  (0, 8482)\t0.19536029174634315\n",
            "  (0, 25299)\t0.4154768420940183\n",
            "  (0, 29994)\t0.5082172479277967\n",
            "  (0, 55552)\t0.4693431009849115\n",
            "  (0, 4498)\t0.24978150512925987\n",
            "----------------------\n",
            "Clean Text 3:   (0, 8191)\t0.27183059233565066\n",
            "  (0, 91525)\t0.29629588709424837\n",
            "  (0, 1094)\t0.47814087848128434\n",
            "  (0, 52809)\t0.2564752834321436\n",
            "  (0, 62486)\t0.1729541689006465\n",
            "  (0, 31833)\t0.32764807807020724\n",
            "  (0, 8849)\t0.47814087848128434\n",
            "  (0, 31162)\t0.3617635138401959\n",
            "  (0, 29792)\t0.1592262533730339\n",
            "  (0, 4498)\t0.14767416391414442\n",
            "----------------------\n",
            "Clean Text 4:   (0, 89748)\t0.33096787096105884\n",
            "  (0, 115555)\t0.264307758616284\n",
            "  (0, 57758)\t0.16550426594427647\n",
            "  (0, 69191)\t0.13802887869790276\n",
            "  (0, 89770)\t0.24208100191948312\n",
            "  (0, 67953)\t0.5847856949249174\n",
            "  (0, 20937)\t0.12959960071604348\n",
            "  (0, 44171)\t0.37825606190200767\n",
            "  (0, 4746)\t0.46228434796762613\n",
            "----------------------\n",
            "Clean Text 5:   (0, 6564)\t0.2812362738308255\n",
            "  (0, 97788)\t0.2381953731048502\n",
            "  (0, 58932)\t0.22596276314296193\n",
            "  (0, 117226)\t0.37127528704378004\n",
            "  (0, 68536)\t0.3531062131941516\n",
            "  (0, 20577)\t0.2876421037608747\n",
            "  (0, 71092)\t0.2741404157153809\n",
            "  (0, 16209)\t0.23297983682981876\n",
            "  (0, 49455)\t0.2620661798275128\n",
            "  (0, 111122)\t0.22534140644536704\n",
            "  (0, 111342)\t0.32895197741459686\n",
            "  (0, 47350)\t0.15019323359622572\n",
            "  (0, 25766)\t0.15731598429180468\n",
            "  (0, 47710)\t0.22284755641628315\n",
            "  (0, 57758)\t0.11737064633361394\n",
            "----------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "labels = np.array(labels)\n",
        "print(labels.shape)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:27.315198Z",
          "iopub.execute_input": "2024-04-23T13:20:27.315667Z",
          "iopub.status.idle": "2024-04-23T13:20:27.325822Z",
          "shell.execute_reply.started": "2024-04-23T13:20:27.315633Z",
          "shell.execute_reply": "2024-04-23T13:20:27.324840Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0T_fyhmID3R",
        "outputId": "5f9f1bf0-c5e5-4d46-c7e4-c12146fd0241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(105698,)\n",
            "(105698, 118419)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(min(5, len(clean_texts))):\n",
        "    print(f\"Clean Text {i+1}: {labels[i]}\")\n",
        "\n",
        "def sentiment(n):\n",
        "    return 1 if n >= 3 else 0\n",
        "\n",
        "last_labels = [sentiment(label) for label in labels]\n",
        "last_labels = np.array(last_labels)\n",
        "print(\"----------------------\")\n",
        "for i in range(min(5, len(clean_texts))):\n",
        "    print(f\"Clean Text {i+1}: {last_labels[i]}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:27.327201Z",
          "iopub.execute_input": "2024-04-23T13:20:27.327481Z",
          "iopub.status.idle": "2024-04-23T13:20:27.382580Z",
          "shell.execute_reply.started": "2024-04-23T13:20:27.327456Z",
          "shell.execute_reply": "2024-04-23T13:20:27.381715Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hWWfSlLID3R",
        "outputId": "eb0a5b4f-9dea-4f16-d847-034e9d25000e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Text 1: 1\n",
            "Clean Text 2: 4\n",
            "Clean Text 3: 4\n",
            "Clean Text 4: 0\n",
            "Clean Text 5: 3\n",
            "----------------------\n",
            "Clean Text 1: 0\n",
            "Clean Text 2: 1\n",
            "Clean Text 3: 1\n",
            "Clean Text 4: 0\n",
            "Clean Text 5: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, last_labels, test_size=0.2, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(f\"X_train data type: {type(X_train)}\")\n",
        "print(f\"y_train data type: {type(y_train)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:27.383538Z",
          "iopub.execute_input": "2024-04-23T13:20:27.383792Z",
          "iopub.status.idle": "2024-04-23T13:20:27.407264Z",
          "shell.execute_reply.started": "2024-04-23T13:20:27.383770Z",
          "shell.execute_reply": "2024-04-23T13:20:27.406481Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QApBnYY1ID3R",
        "outputId": "c035a0f3-d850-4c9a-e277-c3681dcc7396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(84558, 118419)\n",
            "(21140, 118419)\n",
            "(84558,)\n",
            "(21140,)\n",
            "X_train data type: <class 'scipy.sparse._csr.csr_matrix'>\n",
            "y_train data type: <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n"
      ],
      "metadata": {
        "id": "RkVH6KZ0L5je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"X_train data type: {type(X_train_dense)}\")\n",
        "print(f\"y_train data type: {type(y_train)}\")\n",
        "print(f\"X_test data type: {type(X_test_dense)}\")\n",
        "print(f\"y_test data type: {type(y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trAypoZNL9mQ",
        "outputId": "aad51b35-ed8c-429c-f26b-b261b3d64d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train data type: <class 'numpy.ndarray'>\n",
            "y_train data type: <class 'numpy.ndarray'>\n",
            "X_test data type: <class 'numpy.ndarray'>\n",
            "y_test data type: <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "لسا نحتاج نعدل عليه اذا ضبطت الداتا"
      ],
      "metadata": {
        "id": "M5kfD9ngID3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=X_train_dense.shape[1], output_dim=100, input_length=100))  # Adjust input_length to your desired sequence length\n",
        "model.add(LSTM(units=128))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=2, activation='softmax'))\n",
        "\n",
        "model.build(input_shape=(None, X_train_dense.shape[1]))\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:27.408350Z",
          "iopub.execute_input": "2024-04-23T13:20:27.408670Z",
          "iopub.status.idle": "2024-04-23T13:20:39.490950Z",
          "shell.execute_reply.started": "2024-04-23T13:20:27.408644Z",
          "shell.execute_reply": "2024-04-23T13:20:39.489943Z"
        },
        "trusted": true,
        "id": "i2yGpwz3ID3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:39.492072Z",
          "iopub.execute_input": "2024-04-23T13:20:39.492629Z",
          "iopub.status.idle": "2024-04-23T13:20:39.514344Z",
          "shell.execute_reply.started": "2024-04-23T13:20:39.492600Z",
          "shell.execute_reply": "2024-04-23T13:20:39.513460Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BlUTCEQID3S",
        "outputId": "4547872e-f3c7-4873-872d-046d1709bb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          11841900  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               117248    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11967534 (45.65 MB)\n",
            "Trainable params: 11967534 (45.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train_dense, y_train, epochs=5, batch_size=32)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:39.515629Z",
          "iopub.execute_input": "2024-04-23T13:20:39.516224Z",
          "iopub.status.idle": "2024-04-23T13:20:53.240272Z",
          "shell.execute_reply.started": "2024-04-23T13:20:39.516183Z",
          "shell.execute_reply": "2024-04-23T13:20:53.238686Z"
        },
        "trusted": true,
        "id": "uIDySMm2ID3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test data\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "# Calculate precision, recall, F1 score\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test_argmax = np.argmax(y_test, axis=1)\n",
        "\n",
        "print('Precision:', precision_score(y_test_argmax, y_pred, average='weighted'))\n",
        "print('Recall:', recall_score(y_test_argmax, y_pred, average='weighted'))\n",
        "print('F1 score:', f1_score(y_test_argmax, y_pred, average='weighted'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-23T13:20:53.241257Z",
          "iopub.status.idle": "2024-04-23T13:20:53.241635Z",
          "shell.execute_reply.started": "2024-04-23T13:20:53.241463Z",
          "shell.execute_reply": "2024-04-23T13:20:53.241478Z"
        },
        "trusted": true,
        "id": "tmftkbSnID3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}